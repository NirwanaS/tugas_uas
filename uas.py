# -*- coding: utf-8 -*-
"""UAS.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16FE9y2HZlCBTjgGb_JfAEKksuuRRlUST

Nama : Nirwana Safitri

NIM  : A11.2023.15456
"""

!pip install catboost

# Import lib yang perlu
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.svm import SVC
from google.colab import drive
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.impute import SimpleImputer
from xgboost import XGBClassifier
from catboost import CatBoostClassifier
from sklearn.preprocessing import MinMaxScaler
from imblearn.over_sampling import RandomOverSampler
from sklearn.metrics import confusion_matrix, accuracy_score, mean_squared_error, r2_score

"""**Step 1 : Pengumpulan Data**"""

# Baca dataset code-cell berikut
drive.mount('/content/drive')
water_data = pd.read_csv('/content/drive/MyDrive/water_potability.csv')
water_data

"""**Step 2 : Menelaah Data**"""

water_data.info()

print(f"Jumlah baris dan kolom: {water_data.shape[0]} baris, {water_data.shape[1]} kolom\n")

print("Statistik Deskriptif:")
for col, describe in water_data.describe().items():
    print(f"- {col}: {describe} ")

print("Jumlah nilai unik per kolom:")
for col, unique in water_data.nunique().items():
    print(f"- {col}: {unique} ")

print("Jumlah nilai yang hilang per kolom:")
for col, missing in water_data.isnull().sum().items():
    print(f"- {col}: {missing} nilai hilang")

"""**Step 3 : Validasi dan Visualisasi Data**"""

# Cek missing values
sns.heatmap(water_data.isnull(), cbar=False, cmap='viridis')
plt.title("Visualisasi Missing Values")
plt.show()

# Handle Missing Values dengan Mean
water_data_filled = water_data.fillna(water_data.mean())

# Cek Outlier menggunakan IQR
print("Outlier Detection:")
for column in water_data_filled.columns[:-1]:  # Exclude 'Potability'
    Q1 = water_data_filled[column].quantile(0.25)
    Q3 = water_data_filled[column].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    outliers = water_data_filled[(water_data_filled[column] < lower_bound) | (water_data_filled[column] > upper_bound)]
    print(f"{column}: {len(outliers)} outliers")

imputer = SimpleImputer(strategy='mean')
water_data_imputed = pd.DataFrame(imputer.fit_transform(water_data), columns=water_data.columns)

print("Jumlah sampel sebelum resampling:")
print(water_data["Potability"].value_counts())
print()

# Visualisasi distribusi sebelum resampling
plt.figure(figsize=(6, 4))
sns.countplot(x="Potability", data=water_data , palette="Set2")
plt.title("Distribusi Data Kualitas Air Sebelum Resampling")
plt.xlabel("Potability")
plt.ylabel("Jumlah")
plt.xticks([0, 1])
plt.show()

features = ['ph', 'Hardness', 'Solids', 'Chloramines', 'Sulfate', 'Conductivity', 'Organic_carbon', 'Trihalomethanes', 'Turbidity']
target = 'Potability'
X = water_data[features]
y = water_data[target]

# Resampling untuk mengatasi ketidakseimbangan kelas
ros = RandomOverSampler(random_state=42)
X_resampled, y_resampled = ros.fit_resample(X, y)

# Jumlah sampel setelah resampling
print("Jumlah sampel setelah resampling:")
print(y_resampled.value_counts())

# Visualisasi distribusi setelah resampling
plt.figure(figsize=(6, 4))
sns.countplot(x=y_resampled, palette="Set2")
plt.title("Distribusi Data Kualitas Air Setelah Resampling")
plt.xlabel("Kelas Potability")
plt.ylabel("Jumlah")
plt.xticks([0, 1])
plt.show()

"""**Step 4 : Menentukan Object Data**"""

X = X_resampled
y = y_resampled

"""**Step 5 : Membersihkan Data**"""

# Visualisasi korelasi antar atribut menggunakan heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(water_data_imputed.corr(), annot=True, fmt=".2f", cmap="coolwarm", cbar=True)
plt.title("Heatmap Korelasi Antar Fitur")
plt.show()

# Assuming 'water_data' is your DataFrame
potable_data = water_data_imputed[water_data_imputed['Potability'] == 1]
non_potable_data = water_data_imputed[water_data_imputed['Potability'] == 0]

# Get the list of numerical features (attributes) for plotting
numerical_features = water_data_imputed.select_dtypes(include=['number']).columns.tolist()
numerical_features.remove('Potability')  # Remove 'Potability' as it's the target variable

fig, axes = plt.subplots(3, 3, figsize=(20, 15))
axes = axes.flatten()

for i, feature in enumerate(numerical_features):
    ax = axes[i]

    sns.kdeplot(non_potable_data[feature], ax=ax, label='Not Potable', color='blue', fill=False)


    sns.histplot(potable_data[feature], ax=ax, label='Potable', color='skyblue', stat="density", bins=20)

    ax.set_title(f'Distribution of {feature}')
    ax.set_xlabel(feature)
    ax.set_ylabel('Density')
    ax.legend()

# Hide any unused subplots (if there are fewer than 9 numerical features)
for j in range(len(numerical_features), len(axes)):
    fig.delaxes(axes[j])

plt.tight_layout()  # Adjust spacing between subplots
plt.show()

"""**Step 6 : Konstruksi Data**"""

scaler = MinMaxScaler()
X_resampled_scaled = scaler.fit_transform(X_resampled)

"""**Step 7 : Pemodelan**"""

X_train, X_test, y_train, y_test = train_test_split(X_resampled_scaled, y_resampled, test_size=0.2, random_state=42)

# Model definisi
models = {
    'Random Forest': RandomForestClassifier(random_state=42),
    'Decision Tree': DecisionTreeClassifier(random_state=42),
    'XGBoost': XGBClassifier(eval_metric='logloss', random_state=42),
    'CatBoost': CatBoostClassifier(verbose=0, random_state=42)
}

# Train dan evaluasi model
results_normalized = {}
conf_matrices = {}

for name, model in models.items():
    # Fit dan prediksi
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    # Simpan akurasi dan confusion matrix
    results_normalized[name] = accuracy_score(y_test, y_pred)
    conf_matrices[name] = confusion_matrix(y_test, y_pred)

# Tampilkan gambar confusion matrix
num_models = len(conf_matrices)

for i, (model, matrix) in enumerate(conf_matrices.items()):
    plt.figure(figsize=(6, 4))
    sns.heatmap(matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Not Potable', 'Potable'], yticklabels=['Not Potable', 'Potable'])
    plt.title(f'Confusion Matrix for {model}')
    plt.xlabel('Predicted')
    plt.ylabel('Actual')
    plt.show()  # Menampilkan plot menggunakan matplotlib

"""**Step 8 : Evaluasi**"""

# Sebelum Normalisasi
X_train_raw, X_test_raw, y_train_raw, y_test_raw = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)

results_raw = {}
for name, model in models.items():
    model.fit(X_train_raw, y_train_raw)
    y_pred_raw = model.predict(X_test_raw)
    results_raw[name] = accuracy_score(y_test_raw, y_pred_raw)

# Tampilkan hasil
print("\nAkurasi Sebelum Normalisasi:")
for model, acc in results_raw.items():
    print(f"{model}: {acc*100:.1f}%")

print("\nAkurasi Setelah Normalisasi:")
for model, acc in results_normalized.items():
    print(f"{model}: {acc*100:.0f}%")

"""**KESIMPULAN**


1.   **Perbandingan akurasi**

      *   **Random Forest** = 78 %
      *   **Decision Tree** = 72 %
      *   **XGBoost** = 74%
      *   **CatBoost** = 73%

2.   **Kelebihan & kekurangan**
      *   **Random Forest** = Stabil dan akurat, namun memerlukan sumber daya komputasi lebih besar.
      *   **Decision Tree** = Mudah dipahami, namun rentan overfitting.
      *   **XGBoost** = Cepat dan efisien, tetapi sensitif terhadap parameter.
      *   **CatBoost** = Unggul pada data kategoris, namun lebih lambat dibanding XGBoost.
3.   **Saran**
      *   **Random Forest** = untuk akurasi tertinggi jika performa komputasi tidak menjadi kendala.
      *   **Decision Tree** = untuk analisis awal atau model yang mudah dijelaskan.
      *   **XGBoost** = untuk efisiensi dan jika terdapat banyak fitur kategoris.
      *   **CatBoost** = untuk efisiensi dan jika terdapat banyak fitur kategoris.
"""

